{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Wanted a safe space where I could play with JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.arange(5)\n",
    "isinstance(x, jax.Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleDeviceSharding(device=CpuDevice(id=0), memory_kind=unpinned_host)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.devices()\n",
    "x.sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lambda_=1.05):\n",
    "    return lambda_ * jnp.where(x>0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "selu_jit = jax.jit(selu)\n",
    "print(selu_jit(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(int32[4])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "    print(x)\n",
    "    return x + 1\n",
    "\n",
    "x = jnp.arange(4)\n",
    "result = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[5]. let\n",
       "    b:bool[5] = gt a 0.0\n",
       "    c:f32[5] = exp a\n",
       "    d:f32[5] = mul 1.6699999570846558 c\n",
       "    e:f32[5] = sub d 1.6699999570846558\n",
       "    f:f32[5] = pjit[\n",
       "      name=_where\n",
       "      jaxpr={ lambda ; g:bool[5] h:f32[5] i:f32[5]. let\n",
       "          j:f32[5] = select_n g i h\n",
       "        in (j,) }\n",
       "    ] b a e\n",
       "    k:f32[5] = mul 1.0499999523162842 f\n",
       "  in (k,) }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.arange(5.0)\n",
    "jax.make_jaxpr(selu)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef([*, *, (*, *)])\n",
      "[1, 2, Array([0, 1, 2], dtype=int32), Array([1., 1.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "params = [1, 2, (jnp.arange(3), jnp.ones(2))]\n",
    "\n",
    "print(jax.tree.structure(params))\n",
    "print(jax.tree.leaves(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef({'W': *, 'b': *, 'n': *})\n",
      "[Array([[1., 1.],\n",
      "       [1., 1.]], dtype=float32), Array([0., 0.], dtype=float32), 5]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of parameters\n",
    "params = {'n': 5, 'W': jnp.ones((2, 2)), 'b': jnp.zeros(2)}\n",
    "\n",
    "print(jax.tree.structure(params))\n",
    "print(jax.tree.leaves(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef(CustomNode(namedtuple[Params], [*, *]))\n",
      "[1, 5.0]\n"
     ]
    }
   ],
   "source": [
    "# Named tuple of parameters\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Params(NamedTuple):\n",
    "  a: int\n",
    "  b: float\n",
    "\n",
    "params = Params(1, 5.0)\n",
    "print(jax.tree.structure(params))\n",
    "print(jax.tree.leaves(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `jax.jit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.76 ms ± 1.07 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.key(1701)\n",
    "x = random.normal(key, (1_000_000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835 µs ± 63 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "\n",
    "selu_jit = jit(selu)\n",
    "_ = selu_jit(x) # compiles on first call\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `jax.grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661197 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "\n",
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1965761  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_differences(f, x, eps=1E-3):\n",
    "    return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                      for v in jnp.eye(len(x))])\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts:\n",
    "- interesting that grad wraps around *functions*\n",
    "- would be interesting to have a better idea of what is going on under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.        0.       ]\n",
      " [0.        2.7182817 0.       ]\n",
      " [0.        0.        7.389056 ]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacobian\n",
    "print(jacobian(jnp.exp)(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts: seems like magic that this can adapt to the tensor dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.         -0.        ]\n",
      " [-0.         -0.09085776 -0.        ]\n",
      " [-0.         -0.         -0.07996249]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "    return jit(jacfwd(jacrev(fun)))\n",
    "print(hessian(sum_logistic)(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Array([[ 1.       ,  0.       ,  0.       ,  0.       ],\n",
      "       [ 0.       ,  2.7182817,  0.       ,  0.       ],\n",
      "       [ 0.       ,  0.       ,  7.389056 ,  0.       ],\n",
      "       [ 0.       ,  0.       ,  0.       , 20.085537 ]], dtype=float32), Array([[  7.389056,   0.      ,   0.      ,   0.      ],\n",
      "       [  0.      ,  20.085537,   0.      ,   0.      ],\n",
      "       [  0.      ,   0.      ,  54.598152,   0.      ],\n",
      "       [  0.      ,   0.      ,   0.      , 148.41316 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Lennie testing: this does also work with 'standard container'\n",
    "def list_fn(myl: list):\n",
    "    return jnp.exp(myl[0]) + jnp.exp(myl[1])\n",
    "\n",
    "print(jacobian(list_fn)([jnp.arange(4.0), jnp.arange(2,6.0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working out what vjp does...\n",
    "def f(x, y):\n",
    "    return jnp.sin(x), jnp.cos(y)\n",
    "\n",
    "primals_out, f_vjp = jax.vjp(f, 0.5, 1.0)\n",
    "# evalute the function f at *primals and return value and also a vjp function\n",
    "# this sends cotangent vector with shape of primals_out to tuple of cotangent vectors with shape of primals\n",
    "# i.e. think of deriv of f: R^n -> R^m not as matrix of shape (m,n) but as a function R^m to a linear map R^n -> R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check previous thinking\n",
    "def f(x, y, z):\n",
    "    return jnp.sin(x), y + z\n",
    "\n",
    "primals_out, f_vjp = jax.vjp(f, 0.5, 1.0, 2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- when print `f_vjp` it returns a Partial object\n",
    "- subsections of this printed string include jaxprs (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array(0.47942555, dtype=float32, weak_type=True), Array(3., dtype=float32, weak_type=True))\n",
      "Partial(_HashableCallableShim(functools.partial(<function _vjp_pullback_wrapper at 0x7f5d5de67eb0>, 'f', [ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True)], (PyTreeDef((*, *)), PyTreeDef((*, *, *))))), Partial(_HashableCallableShim(functools.partial(<function vjp.<locals>.unbound_vjp at 0x7f5d5cb293f0>, [(ShapedArray(float32[], weak_type=True), None), (ShapedArray(float32[], weak_type=True), None)], { lambda a:f32[]; b:f32[] c:f32[] d:f32[]. let\n",
      "    e:f32[] = pjit[\n",
      "      name=sin\n",
      "      jaxpr={ lambda ; f:f32[] g:f32[]. let h:f32[] = mul f g in (h,) }\n",
      "    ] b a\n",
      "    i:f32[] = pjit[\n",
      "      name=_add\n",
      "      jaxpr={ lambda ; j:f32[] k:f32[]. let l:f32[] = add j k in (l,) }\n",
      "    ] c d\n",
      "  in (e, i) })), (Array(0.87758255, dtype=float32, weak_type=True),)))\n"
     ]
    }
   ],
   "source": [
    "print(primals_out)\n",
    "print(f_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.87758255, dtype=float32, weak_type=True),\n",
       " Array(2., dtype=float32, weak_type=True),\n",
       " Array(2., dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_vjp((1.0, 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saying: 'what is the derivative in this direction?'  \n",
    "\n",
    "Question to self: why might that be useful?  \n",
    "Think this is because it is what you need for backprop, and saves you having to compute a full Jacobian matrix (which would be big)  \n",
    "I guess this helps reduce the amount of information that needs to be passed around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource: really nice explanation in https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional extension: try out more of those ways to understand what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `jax.vmap()`\n",
    "Auto-vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'jax.random' from '/store/DPMMS/ww347/ML_reproductions/jax_ecosystem/local_conda_env_jax/lib/python3.10/site-packages/jax/random.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1, key2 = random.split(key)\n",
    "mat = random.normal(key1, (150, 100))\n",
    "batched_x = random.normal(key2, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "    return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "1.08 ms ± 188 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "    return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n",
      "23.3 µs ± 1.01 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "    return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    naively_batched_apply_matrix(batched_x),\n",
    "    batched_apply_matrix(batched_x), \n",
    "    atol=1E-6, rtol=1E-3,\n",
    ")\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "32 µs ± 1.13 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import vmap\n",
    "\n",
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "    return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    naively_batched_apply_matrix(batched_x),\n",
    "    vmap_batched_apply_matrix(batched_x), \n",
    "    atol=1E-6, rtol=1E-3,\n",
    ")\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for some reason really did need to wind down the rtol to avoid an assertion error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: this does seem nice and naively brings down execution times by a factor of 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just-in-time compilation\n",
    "https://jax.readthedocs.io/en/latest/jit-compilation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[]. let\n",
      "    b:f32[] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = div b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "global_list = []\n",
    "def log2(x):\n",
    "    global_list.append(x)\n",
    "    ln_x = jnp.log(x)\n",
    "    ln_2 = jnp.log(2.0)\n",
    "    return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2)(3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: nothing about side-effect.  \n",
    "Feature not bug!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'When tracing, JAX wraps each argument by a tracer object. These tracers then record all JAX operations performed on them during the function call (which happens in regular Python). Then, JAX uses the tracer records to reconstruct the entire function. The output of that reconstruction is the jaxpr. Since the tracers do not record the Python side-effects, they do not appear in the jaxpr. However, the side-effects still happen during the trace itself.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printed x: Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n",
      "\n",
      "jaxpr: { lambda ; a:f32[]. let\n",
      "    b:f32[] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = div b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_with_print(x):\n",
    "    print(\"printed x:\", x)\n",
    "    ln_x = jnp.log(x)\n",
    "    ln_2 = jnp.log(2.0)\n",
    "    return ln_x / ln_2\n",
    "\n",
    "jaxpr = jax.make_jaxpr(log2_with_print)(3.0)\n",
    "print('\\njaxpr:', jaxpr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so printing happens when the jaxpr is evaluated; but doesn't show up in the jaxpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'A key thing to understand is that a jaxpr captures the function as executed on the parameters given to it. For example, if we have a Python conditional, the jaxpr will only know about the branch we take:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 example\n",
      "{ lambda ; a:f32[2]. let  in (a,) }\n",
      "Rank 2 example\n",
      "{ lambda ; a:f32[2,2]. let\n",
      "    b:f32[2,2] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n",
      "    e:f32[2,2] = div b d\n",
      "  in (e,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_if_rank_2(x):\n",
    "    if x.ndim == 2:\n",
    "        ln_x = jnp.log(x)\n",
    "        ln_2 = jnp.log(2.0)\n",
    "        return ln_x / ln_2\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "print('Rank 1 example')\n",
    "print(jax.make_jaxpr(log2_if_rank_2)(jnp.array([3.0, 4.0])))\n",
    "\n",
    "print('Rank 2 example')\n",
    "print(jax.make_jaxpr(log2_if_rank_2)(jnp.array([[3.0, 4.0], [5.0, 6.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT compiling a function\n",
    "Just repeated from jit section of quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why can't we just JIT everything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function f at /tmp/ipykernel_2458417/2494647410.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x\n\u001b[0;32m----> 9\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[58], line 4\u001b[0m, in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/store/DPMMS/ww347/ML_reproductions/jax_ecosystem/local_conda_env_jax/lib/python3.10/site-packages/jax/_src/core.py:1547\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1547\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function f at /tmp/ipykernel_2458417/2494647410.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "# Condition on value of x.\n",
    "\n",
    "def f(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 2 * x\n",
    "    \n",
    "jax.jit(f)(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function g at /tmp/ipykernel_2458417/3570161443.py:2 for jit. This concrete value was not available in Python because it depends on the value of the argument n.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m----> 8\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[59], line 4\u001b[0m, in \u001b[0;36mg\u001b[0;34m(x, n)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(x, n):\n\u001b[1;32m      3\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m i\u001b[38;5;241m<\u001b[39m n:\n\u001b[1;32m      5\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m i\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/store/DPMMS/ww347/ML_reproductions/jax_ecosystem/local_conda_env_jax/lib/python3.10/site-packages/jax/_src/core.py:1547\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1547\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function g at /tmp/ipykernel_2458417/3570161443.py:2 for jit. This concrete value was not available in Python because it depends on the value of the argument n.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "# While loop conditioned on x and n\n",
    "def g(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i += 1\n",
    "    return x + i\n",
    "\n",
    "jax.jit(g)(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:\n",
    "- Can rewrite code to avoid conditionals on value\n",
    "- Use [Control flow operators](https://jax.readthedocs.io/en/latest/jax.lax.html#lax-control-flow) like jax.lax.cond(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(30, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# While loop conditioned on x and n with a jitted body.\n",
    "\n",
    "@jax.jit\n",
    "def loop_body(prev_i):\n",
    "    return prev_i + 1\n",
    "\n",
    "def g_inner_jitted(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i = loop_body(i)\n",
    "    return x + i\n",
    "\n",
    "g_inner_jitted(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(10, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: implement same function but using jax.lax.cond\n",
    "from jax.lax import while_loop\n",
    "\n",
    "# my version\n",
    "def g_lax(x, n):\n",
    "    i = 0\n",
    "    while_loop(cond_fun=lambda i: i < n,\n",
    "               body_fun=lambda i: i + 1,\n",
    "               init_val=i)\n",
    "    return x + i\n",
    "\n",
    "jax.jit(g_lax)(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_lax_gpt4(x, n):\n",
    "    def body_fun(i):\n",
    "        return i + 1\n",
    "    def cond_fun(i):\n",
    "        return i < n\n",
    "    i = jax.lax.while_loop(cond_fun, body_fun, 0)\n",
    "    return x + i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before adding in the block until ready I got\n",
    "```\n",
    "Jitted gpt4 version\n",
    "10.9 µs ± 508 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
    "Jitted version\n",
    "7.28 µs ± 700 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
    "Original version\n",
    "652 ns ± 72.7 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
    "Inner jitted version\n",
    "632 µs ± 9.04 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i += 1\n",
    "    return x + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jitted gpt4 version\n",
      "12.1 µs ± 1.08 µs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "Jitted version\n",
      "7.86 µs ± 128 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "Original version\n",
      "575 ns ± 8.62 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "Inner jitted version\n",
      "642 µs ± 2.19 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "gitted = jax.jit(g_lax)\n",
    "gitted(10, 20)\n",
    "\n",
    "gitted_gpt4 = jax.jit(g_lax_gpt4)\n",
    "gitted_gpt4(10, 20)\n",
    "\n",
    "print('Jitted gpt4 version')\n",
    "%timeit gitted_gpt4(10, 20).block_until_ready()\n",
    "\n",
    "print('Jitted version')\n",
    "%timeit gitted(10, 20).block_until_ready()\n",
    "\n",
    "print('Original version')\n",
    "%timeit g(10, 20)\n",
    "\n",
    "print('Inner jitted version')\n",
    "%timeit g_inner_jitted(10, 20).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is somewhat surprising: the original version is far faster than the other options!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation from GPT4: the jit compilation happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While loop conditioned on x and n\n",
    "def g(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i += 1\n",
    "    return x + i\n",
    "\n",
    "# While loop conditioned on x and n with a jitted body.\n",
    "@jax.jit\n",
    "def loop_body(prev_i):\n",
    "    return prev_i + 1\n",
    "\n",
    "def g_inner_jitted(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i = loop_body(i)\n",
    "    return x + i\n",
    "\n",
    "# My idea: use jax.lax\n",
    "from jax.lax import while_loop\n",
    "\n",
    "def g_lax(x, n):\n",
    "    i = 0\n",
    "    while_loop(cond_fun=lambda i: i < n,\n",
    "               body_fun=lambda i: i + 1,\n",
    "               init_val=i)\n",
    "    return x + i\n",
    "\n",
    "gitted = jax.jit(g_lax)\n",
    "gitted(10, 20)\n",
    "\n",
    "print('Jitted version')\n",
    "%timeit gitted(10, 20)\n",
    "\n",
    "print('Original version')\n",
    "%timeit g(10, 20)\n",
    "\n",
    "print('Inner jitted version')\n",
    "%timeit g_inner_jitted(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original version\n",
      "2.3959092549048364\n",
      "Inner jitted version\n",
      "2.0549116819165647\n",
      "JAX while_loop jitted version\n",
      "0.410714193014428\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax.lax import while_loop\n",
    "\n",
    "# Initial matrix A and power n\n",
    "A = jnp.array(jax.random.normal(jax.random.PRNGKey(0), (20, 20)))\n",
    "power = 20\n",
    "\n",
    "# Original version using a simple loop\n",
    "def matrix_power(A, n):\n",
    "    result = A\n",
    "    for _ in range(1, n):\n",
    "        result = result @ A\n",
    "    return result\n",
    "\n",
    "# Inner jitted version with jitted multiplication\n",
    "@jit\n",
    "def multiply_matrices(B, A):\n",
    "    return B @ A\n",
    "\n",
    "def matrix_power_inner_jitted(A, n):\n",
    "    result = A\n",
    "    for _ in range(1, n):\n",
    "        result = multiply_matrices(result, A)\n",
    "    return result\n",
    "\n",
    "# JAX while_loop version\n",
    "def matrix_power_lax(A, n):\n",
    "    def body_fun(carry):\n",
    "        result, A, i = carry\n",
    "        return (result @ A, A, i + 1)\n",
    "    \n",
    "    def cond_fun(carry):\n",
    "        _, _, i = carry\n",
    "        return i < n\n",
    "\n",
    "    final_result, _, _ = while_loop(cond_fun, body_fun, (A, A, 1))\n",
    "    return final_result\n",
    "\n",
    "# Jitting the while_loop version\n",
    "matrix_power_lax_jitted = jit(matrix_power_lax)\n",
    "\n",
    "# Timing and comparing the versions\n",
    "import timeit\n",
    "\n",
    "print('Original version')\n",
    "print(timeit.timeit('matrix_power(A, power)', globals=globals(), number=10000))\n",
    "\n",
    "print('Inner jitted version')\n",
    "print(timeit.timeit('matrix_power_inner_jitted(A, power)', globals=globals(), number=10000))\n",
    "\n",
    "print('JAX while_loop jitted version')\n",
    "print(timeit.timeit('matrix_power_lax_jitted(A, power)', globals=globals(), number=10000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough there does not seem to be a massive difference here.  \n",
    "But at least the ordering is now as one might expect / hope (the more complicated version is in fact better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making arguments static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can tell JAX to re-compile the function for each new value of the specified static input.  \n",
    "(Good if limited set of static values). \n",
    "\n",
    "To do this when using jit as a decorator can use functools.partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jit called in a loop with partials:\n",
      "311 ms ± 18.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "jit called in a loop with lambdas:\n",
      "290 ms ± 2.72 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "jit called in a loop with caching:\n",
      "2.56 ms ± 24.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def unjitted_loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted_partial(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # Don't do this! each time the partial returns\n",
    "    # a function with different hash\n",
    "    i = jax.jit(partial(unjitted_loop_body))(i)\n",
    "  return x + i\n",
    "\n",
    "def g_inner_jitted_lambda(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # Don't do this!, lambda will also return\n",
    "    # a function with a different hash\n",
    "    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n",
    "  return x + i\n",
    "\n",
    "def g_inner_jitted_normal(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # this is OK, since JAX can find the\n",
    "    # cached, compiled function\n",
    "    i = jax.jit(unjitted_loop_body)(i)\n",
    "  return x + i\n",
    "\n",
    "print(\"jit called in a loop with partials:\")\n",
    "%timeit g_inner_jitted_partial(10, 20).block_until_ready()\n",
    "\n",
    "print(\"jit called in a loop with lambdas:\")\n",
    "%timeit g_inner_jitted_lambda(10, 20).block_until_ready()\n",
    "\n",
    "print(\"jit called in a loop with caching:\")\n",
    "%timeit g_inner_jitted_normal(10, 20).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this uses the block_until_ready  \n",
    "Let's see if that tweaks the answer in prev code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_conda_env_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
